# âš¡ Sparse Convolution Benchmarking with PyTorch  

This project demonstrates a **sparse convolution implementation** in Python ğŸ using **NumPy, SciPy, and PyTorch**.  
It compares a **pure Python sparse convolution algorithm** with **PyTorchâ€™s optimized conv2d** and analyzes performance at different sparsity levels.  

---

## ğŸš€ Features  
- ğŸ§® Implements **Direct Sparse Algorithm** (ICLR â€˜17 paper inspired)  
- ğŸ” Converts filters into **sparse weight matrices**  
- ğŸ“Š Benchmarks execution times at **different sparsity levels**  
- âš¡ Compares with **PyTorchâ€™s conv2d** performance  
- âœ… Verifies correctness of outputs against PyTorch  

---

## ğŸ“‚ Project Structure  
- `sparse_convolution()` â†’ Implements convolution using Python loops  
- `generate_sparse_weight()` â†’ Builds sparse weight matrices  
- **Benchmarking results** included for:  
  - 50% sparsity â†’ ~25s  
  - 90% sparsity â†’ ~9s  
  - 100% sparsity â†’ ~38ms  
  - PyTorch conv2d â†’ ~4ms (constant)  

---

## ğŸ“Š Performance Overview  

| Sparsity Level | Python Sparse Convolution â±ï¸ | PyTorch conv2d â±ï¸ |
|----------------|-------------------------------|-------------------|
| 50%            | ~25 s                         | ~4 ms             |
| 90%            | ~9 s                          | ~4 ms             |
| 100%           | ~38 ms                        | ~4 ms             |

âœ… Algorithm produces **correct results** (difference = 0.0)  
âš¡ Huge speedup observed with **higher sparsity**  

---

## ğŸ› ï¸ How to Run  

1. Install dependencies:  
```bash
pip install torch numpy scipy
# insurance
